{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigen vectors\n",
    "\n",
    "> Eigen vectors are unit vectors, which means that their length or magnitude is equal to 1.0.<br>\n",
    "  They are often referred as right vectors, which simply means a column vector (as opposed to a row vector or a left vector).<br>\n",
    "  A right-vector is a vector as we understand them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> An eigenvector is a vector whose direction remains unchanged when a linear transformation is applied to it.<br> Consider the image below in which three vectors are shown.<br> The green square is only drawn to illustrate the linear transformation that is applied to each of these three vectors.\n",
    "\n",
    "<img src=\"images_detail/eigenvectors.png\" style=\"width:300px; background:white;\"/>\n",
    "\n",
    "> Eigenvectors (red) do not change direction when a linear transformation (e.g. scaling) is applied to them. Other vectors (yellow) do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">example of initialising an eigen vector\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "Eigen vectors from numpy library\n",
      "\n",
      "[[-0.23197069 -0.78583024  0.40824829]\n",
      " [-0.52532209 -0.08675134 -0.81649658]\n",
      " [-0.8186735   0.61232756  0.40824829]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy.linalg import eig\n",
    "# define matrix\n",
    "Arr= array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(Arr)\n",
    "# calculating the values\n",
    "values, vectors = eig(Arr)\n",
    "print(\"\\nEigen vectors from numpy library\\n\")\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigen values\n",
    "\n",
    "> Eigenvalues are coefficients applied to eigen-vectors that give the vectors their length or magnitude.<br>\n",
    "  For example, a negative eigenvalue may reverse the direction of the eigenvector as part of scaling it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> example from the code above\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.61168440e+01 -1.11684397e+00 -9.75918483e-16]\n"
     ]
    }
   ],
   "source": [
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> confirming the eigen vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -3.73863537  -8.46653421 -13.19443305]\n",
      "[ -3.73863537  -8.46653421 -13.19443305]\n"
     ]
    }
   ],
   "source": [
    "B = Arr.dot(vectors[:, 0])\n",
    "print(B)\n",
    "\n",
    "C = vectors[:, 0] * values[0]\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The example multiplies the original matrix with the first eigenvector and compares it to the first eigenvector multiplied by the first eigenvalue.<br>\n",
    "Running the example prints the results of these two multiplications that show the same resulting vector, as we would expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>\n",
    "# Regularisation\n",
    "\n",
    ">before i start explaing regularisation let me explain underfitting and over fitting\n",
    "***\n",
    "### Underfitting:\n",
    "   \n",
    "   >if model is not doing well with training set,this condition is known as high bias or\n",
    "\t underfitting.\n",
    "\n",
    "    - Why Underfitting:\n",
    "    -----------------\t\n",
    "\t>less no. of features\n",
    "\t>features are not scaled\n",
    "\t>less no. of samples\n",
    "\t>hyper-parameters are not tuned properly\n",
    "\t>algo is not sensitive to dateset\t\n",
    "\n",
    "    - How to increase features:\n",
    "    ------------------------\n",
    "\t>we may use polynomial features to add extra features\n",
    "\t>PolynomialFeatures class can be used to implement this concept    \n",
    "***\n",
    "***\n",
    "### Overfitting:\n",
    "\n",
    "  >And if a model is not doing well with the testing dataset, this condition is known as ‘High-variance’ or over fitting\n",
    "  \n",
    "  - and to tackel this high variance problem we use two methods:\n",
    "    >reducing the number of features<br>\n",
    "    >Regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Consider the case of fitting a function of degree 10. The hypothesis would be like:<br><br>\n",
    "<img src=\"images_detail/hypothesis.png\" style=\"width:400px\" /> <br>\n",
    "If we penalize theta_10 and make it very small(almost equal to zero) then the hypothesis would be reduced to an equation of degree 9 which will be the optimal fit for the data.<br>\n",
    "This is a general idea behind Regularization. Instead of reducing just one parameter, we will be penalizing all the parameters. This will give rise to a simpler hypothesis that is less prone to overfitting.\n",
    "\n",
    "> the cost function to any regression model is:<br>\n",
    "<img src=\"images_detail/cost_fun.png\" style=\"width:400px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">There are generally two type of regularisation\n",
    "\n",
    "### 1. L1 regularization or LASSO regression.\n",
    "### 2. L2 regularization or Ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Lasso regression\n",
    "\n",
    "<img src=\"images_detail/lasso.jpg\" style=\"width:800px\" />\n",
    "\n",
    "***\n",
    "<br>\n",
    "\n",
    "***\n",
    "# Ridge regression\n",
    "\n",
    "<img src=\"images_detail/ridge.jpg\" style=\"width:800px\" />\n",
    "\n",
    "***\n",
    "\n",
    "- for practical code implementatin of lasso and ridge regression model use:<br>\n",
    "<code>from sklearn.linear_model import Ridge,Lasso <br>\n",
    "rd=Ridge(alpha=30)<br>\n",
    "ls=Lasso(alpha=30)</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Sigmoid, Tanh AS A BASIC FUNCTION\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Sigmoid:\n",
    "\n",
    ">The input to the function is transformed into a value between 0.0 and 1.0.\n",
    "\n",
    ">For a long time, through the early 1990s, it was the default activation used on neural networks.\n",
    "\n",
    "<CODE>values=[5.0,0.0,-2.3]\n",
    "print(tf.nn.sigmoid(values))</CODE>\n",
    "\n",
    "***\n",
    "\n",
    "## Tanh:(hyperbolic tangent)\n",
    "\n",
    "\n",
    ">The input to the function is transformed into a value between -1.0 and 1.0.\n",
    "\n",
    ">In the later 1990s and through the 2000s, the tanh function was preferred over the sigmoid activation function.\n",
    "\n",
    "- Note:the hyperbolic tangent activation function typically performs better than the logistic sigmoid.\n",
    "\t\n",
    "<CODE>values=[5.0,0.0,-2.3]\n",
    "print(tf.nn.tanh(values))</CODE>\n",
    "\n",
    "***\n",
    "\n",
    "- Problems with both:\n",
    "\n",
    ">A general problem with both the sigmoid and tanh functions is that they saturate. This means that large values snap to 1.0 and small values snap to -1 or 0 for tanh and sigmoid respectively. <br>\n",
    "saturate: output value is bound in a range and not free\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
